{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2_clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYuoq7g4gF0j",
        "colab_type": "code",
        "outputId": "d1facf5f-67aa-4f01-91fb-312be7e8304f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? Y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGuPf5IBg3AK",
        "colab_type": "code",
        "outputId": "2e27fc1d-819b-4b7c-f639-708ac4304c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 42kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 60.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/81/d1e7d9974ba7c886f6d133a8baae18cb8d92b2d09bcc4f46328306825de0/google_auth-1.7.0-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (41.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.3)\n",
            "\u001b[31mERROR: tensorboard 2.0.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed google-auth-1.7.0 tensorboard-2.0.1 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fImDS0q8qy-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import balanced_accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U19QJKfLkNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def submission(y_test_pred, sample,submission_name):\n",
        "  #creates a submission file given Original test set for ID and the predictions\n",
        "  test_id=(sample.to_numpy())[:,0]\n",
        "\n",
        "  submission={\"id\": test_id ,\n",
        "              \"y\": y_test_pred.flatten()\n",
        "             }\n",
        "  submission=pd.DataFrame(submission)\n",
        "  print(submission.head())\n",
        "\n",
        "  submission.to_csv(submission_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3rUTo6fix3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model1(X_train, y_train, batch_size,epochs):\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(100,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(80,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(60,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(40,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.5),\n",
        "    tf.keras.layers.Dense(20,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(3,activation=\"softmax\")\n",
        "    \n",
        "  ])\n",
        "    \n",
        "\n",
        "  model.compile(optimizer=\"adam\",\n",
        "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])\n",
        "  model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=0)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ-G4gDYi0sJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model2(X_train, y_train, batch_size,epochs):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(500,activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(200,activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(50,activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(30,activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(20,activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dropout(rate=0.5),\n",
        "    tf.keras.layers.Dense(5,activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(3,activation=\"softmax\")\n",
        "    \n",
        "  ])\n",
        "    \n",
        "\n",
        "  model.compile(optimizer=\"adam\",\n",
        "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])\n",
        "  model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=0)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4hra5EAkLGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model3(X_train, y_train, batch_size,epochs):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(30,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(20,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(10,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(20,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.3),\n",
        "    tf.keras.layers.Dense(10,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(rate=0.5),\n",
        "    tf.keras.layers.Dense(4,activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(3,activation=\"softmax\")\n",
        "    \n",
        "  ])\n",
        "    \n",
        "\n",
        "  model.compile(optimizer=\"adam\",\n",
        "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])\n",
        "  model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=0)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqaVrqYSiT6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_all_nets(X_train_copy,y_train_copy,batch_size,epochs):\n",
        "\n",
        "  features_0=X_train_copy[np.where(y_train_copy==0)[0]]\n",
        "  features_2=X_train_copy[np.where(y_train_copy==2)[0]]\n",
        "  y_0=y_train_copy[np.where(y_train_copy==0)[0]]\n",
        "  y_2=y_train_copy[np.where(y_train_copy==2)[0]]\n",
        "  X_label1=X_train_copy[np.where(y_train_copy==1)[0]]\n",
        "  y_1abel1=y_train_copy[np.where(y_train_copy==1)[0]]\n",
        "  X_label1, y_1abel1= shuffle(X_label1, y_1abel1, random_state=0)\n",
        "\n",
        "  X_train_augmented=tf.concat([X_train_copy,features_0 ,features_0,features_0,features_0],axis=0) \n",
        "  #along the rows\n",
        "  y_train_augmented=tf.concat([y_train_copy.flatten(),tf.zeros(4*features_0.shape[0],dtype=\"int64\")],axis=0)\n",
        "  \n",
        "  X_train_augmented=tf.concat([X_train_augmented,features_2,features_2,features_2,features_2],axis=0) #along the rows\n",
        "  y_train_augmented=tf.concat([y_train_augmented,2*tf.ones(4*features_2.shape[0],dtype=\"int64\")],0)\n",
        "\n",
        "  X_train_augmented=tf.random.shuffle(X_train_augmented,seed=8).numpy()\n",
        "  y_train_augmented=tf.random.shuffle(y_train_augmented,seed=8).numpy().reshape(y_train_augmented.shape[0],1)\n",
        "  \n",
        "  #Getting the datasets chunks of label 1\n",
        "  label1_chunks=[]\n",
        "  chunks_size=int(X_label1.shape[0]/6)\n",
        "  for i in range(6):\n",
        "    label1_chunks.append((X_label1[chunks_size*i:chunks_size*(i+1)],y_1abel1[chunks_size*i:chunks_size*(i+1)]))\n",
        "  \n",
        "  #preparing training sets for models\n",
        "  model_sets=[(X_train_augmented,y_train_augmented)]\n",
        "  for i in range(6):\n",
        "    model_set=((np.concatenate((label1_chunks[i][0],features_0,features_2),axis=0),np.concatenate((label1_chunks[i][1],y_0,y_2),axis=0)\n",
        " ))\n",
        "    model_sets.append(shuffle(model_set[0],model_set[1],random_state=1))\n",
        "\n",
        "  models=[]\n",
        "  for model_set in model_sets:\n",
        "    for epoch in epochs:\n",
        "      models.append(train_model1(model_set[0],model_set[1],batch_size,epoch))\n",
        "      models.append(train_model2(model_set[0],model_set[1],batch_size,epoch))\n",
        "      models.append(train_model3(model_set[0],model_set[1],batch_size,epoch))\n",
        "\n",
        "      print(\"just trained 3 nets\")\n",
        "\n",
        "  return models\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwLUWGOiqZFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=pd.read_csv(\"/content/X_train.csv\")\n",
        "X_test=pd.read_csv(\"/content/X_test.csv\")\n",
        "y_train=pd.read_csv(\"/content/y_train.csv\")\n",
        "sample=pd.read_csv(\"/content/sample.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBkpV4z1qxdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_copy=X_train.to_numpy()[:,1:]\n",
        "y_train_copy=y_train.to_numpy()[:,1:]\n",
        "X_test_copy=X_test.to_numpy()[:,1:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZabubI4rBk8",
        "colab_type": "code",
        "outputId": "7cfa2175-1e9c-47b2-c77f-fa4f905694b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "models=[]\n",
        "for i in range(2):\n",
        "  models.append(train_model1(X_train_copy,y_train_copy,10,10))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4800 samples\n",
            "Epoch 1/10\n",
            "4800/4800 [==============================] - 2s 470us/sample - loss: 0.6164 - sparse_categorical_accuracy: 0.7460\n",
            "Epoch 2/10\n",
            "4800/4800 [==============================] - 2s 353us/sample - loss: 0.5492 - sparse_categorical_accuracy: 0.7740\n",
            "Epoch 3/10\n",
            "4800/4800 [==============================] - 2s 347us/sample - loss: 0.5269 - sparse_categorical_accuracy: 0.7869\n",
            "Epoch 4/10\n",
            "4800/4800 [==============================] - 2s 366us/sample - loss: 0.4978 - sparse_categorical_accuracy: 0.7937\n",
            "Epoch 5/10\n",
            "4800/4800 [==============================] - 2s 370us/sample - loss: 0.4890 - sparse_categorical_accuracy: 0.7998\n",
            "Epoch 6/10\n",
            "4800/4800 [==============================] - 2s 388us/sample - loss: 0.4666 - sparse_categorical_accuracy: 0.8127\n",
            "Epoch 7/10\n",
            "4800/4800 [==============================] - 2s 387us/sample - loss: 0.4458 - sparse_categorical_accuracy: 0.8121\n",
            "Epoch 8/10\n",
            "4800/4800 [==============================] - 2s 374us/sample - loss: 0.4244 - sparse_categorical_accuracy: 0.8223\n",
            "Epoch 9/10\n",
            "4800/4800 [==============================] - 2s 374us/sample - loss: 0.4082 - sparse_categorical_accuracy: 0.8283\n",
            "Epoch 10/10\n",
            "4800/4800 [==============================] - 2s 355us/sample - loss: 0.3863 - sparse_categorical_accuracy: 0.8385\n",
            "Train on 4800 samples\n",
            "Epoch 1/10\n",
            "4800/4800 [==============================] - 2s 504us/sample - loss: 0.5919 - sparse_categorical_accuracy: 0.7556\n",
            "Epoch 2/10\n",
            "4800/4800 [==============================] - 2s 343us/sample - loss: 0.5488 - sparse_categorical_accuracy: 0.7831\n",
            "Epoch 3/10\n",
            "4800/4800 [==============================] - 2s 352us/sample - loss: 0.5106 - sparse_categorical_accuracy: 0.7921\n",
            "Epoch 4/10\n",
            "4800/4800 [==============================] - 2s 359us/sample - loss: 0.4932 - sparse_categorical_accuracy: 0.7981\n",
            "Epoch 5/10\n",
            "4800/4800 [==============================] - 2s 358us/sample - loss: 0.4781 - sparse_categorical_accuracy: 0.8008\n",
            "Epoch 6/10\n",
            "4800/4800 [==============================] - 2s 350us/sample - loss: 0.4623 - sparse_categorical_accuracy: 0.8094\n",
            "Epoch 7/10\n",
            "4800/4800 [==============================] - 2s 363us/sample - loss: 0.4329 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 8/10\n",
            "4800/4800 [==============================] - 2s 340us/sample - loss: 0.4100 - sparse_categorical_accuracy: 0.8267\n",
            "Epoch 9/10\n",
            "4800/4800 [==============================] - 2s 345us/sample - loss: 0.3983 - sparse_categorical_accuracy: 0.8294\n",
            "Epoch 10/10\n",
            "4800/4800 [==============================] - 2s 352us/sample - loss: 0.3649 - sparse_categorical_accuracy: 0.8473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JHBlHrbt6Ia",
        "colab_type": "code",
        "outputId": "0f78c7ae-0b7c-436e-959b-86edbd140f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "models=train_all_nets(X_train_copy,y_train_copy,10,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9600 samples\n",
            "9600/9600 [==============================] - 4s 408us/sample - loss: 0.8264 - sparse_categorical_accuracy: 0.6180\n",
            "Train on 9600 samples\n",
            "9600/9600 [==============================] - 7s 690us/sample - loss: 0.8370 - sparse_categorical_accuracy: 0.6271\n",
            "just trained 2 nets\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 1s 640us/sample - loss: 0.9347 - sparse_categorical_accuracy: 0.5000\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 2s 964us/sample - loss: 0.8996 - sparse_categorical_accuracy: 0.5672\n",
            "just trained 2 nets\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 1s 648us/sample - loss: 0.9993 - sparse_categorical_accuracy: 0.4750\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 2s 1ms/sample - loss: 0.9290 - sparse_categorical_accuracy: 0.5483\n",
            "just trained 2 nets\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 1s 656us/sample - loss: 0.9539 - sparse_categorical_accuracy: 0.5194\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 2s 959us/sample - loss: 0.9417 - sparse_categorical_accuracy: 0.5656\n",
            "just trained 2 nets\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 1s 664us/sample - loss: 0.9663 - sparse_categorical_accuracy: 0.4878\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 2s 961us/sample - loss: 0.9428 - sparse_categorical_accuracy: 0.5544\n",
            "just trained 2 nets\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 1s 649us/sample - loss: 0.9620 - sparse_categorical_accuracy: 0.4756\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 2s 1ms/sample - loss: 0.9753 - sparse_categorical_accuracy: 0.5339\n",
            "just trained 2 nets\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 1s 652us/sample - loss: 0.9996 - sparse_categorical_accuracy: 0.4639\n",
            "Train on 1800 samples\n",
            "1800/1800 [==============================] - 2s 969us/sample - loss: 0.9685 - sparse_categorical_accuracy: 0.5433\n",
            "just trained 2 nets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3fzxHdLCrmb",
        "colab_type": "code",
        "outputId": "df8ef544-0a59-4d11-e567-a22f443ebe80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "models[].predict(X_train_copy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.0444042e-04, 9.9977070e-01, 2.4901854e-05],\n",
              "       [5.3286117e-01, 2.1614382e-01, 2.5099501e-01],\n",
              "       [3.4669545e-01, 5.6994176e-01, 8.3362788e-02],\n",
              "       ...,\n",
              "       [5.0587546e-02, 9.3897748e-01, 1.0435058e-02],\n",
              "       [4.6211627e-01, 1.6684051e-01, 3.7104321e-01],\n",
              "       [4.2924425e-01, 7.4483588e-02, 4.9627215e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP8kw_VnEavw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVomwrwXFLZV",
        "colab_type": "code",
        "outputId": "52e8e6ba-edb4-4ed1-9181-b9b3e4837cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "models=train_all_nets(X_train_cv,y_train_cv,10,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6796 samples\n",
            "6796/6796 [==============================] - 3s 442us/sample - loss: 0.8622 - sparse_categorical_accuracy: 0.5917\n",
            "Train on 6796 samples\n",
            "6796/6796 [==============================] - 5s 771us/sample - loss: 0.8452 - sparse_categorical_accuracy: 0.6258\n",
            "just trained 2 nets\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 821us/sample - loss: 0.9749 - sparse_categorical_accuracy: 0.4659\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 1ms/sample - loss: 0.9030 - sparse_categorical_accuracy: 0.5859\n",
            "just trained 2 nets\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 810us/sample - loss: 0.9989 - sparse_categorical_accuracy: 0.4800\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 1ms/sample - loss: 0.9733 - sparse_categorical_accuracy: 0.5271\n",
            "just trained 2 nets\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 806us/sample - loss: 1.0417 - sparse_categorical_accuracy: 0.4322\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 1ms/sample - loss: 1.0743 - sparse_categorical_accuracy: 0.5404\n",
            "just trained 2 nets\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 820us/sample - loss: 1.0545 - sparse_categorical_accuracy: 0.4259\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 1ms/sample - loss: 0.9783 - sparse_categorical_accuracy: 0.5318\n",
            "just trained 2 nets\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 843us/sample - loss: 0.9683 - sparse_categorical_accuracy: 0.4847\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 1ms/sample - loss: 0.9314 - sparse_categorical_accuracy: 0.5514\n",
            "just trained 2 nets\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 1s 802us/sample - loss: 0.9748 - sparse_categorical_accuracy: 0.4769\n",
            "Train on 1275 samples\n",
            "1275/1275 [==============================] - 2s 1ms/sample - loss: 0.9356 - sparse_categorical_accuracy: 0.5522\n",
            "just trained 2 nets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLQh1ldbNabx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(models,X_test):\n",
        "  predictions_sum=0\n",
        "  for model in models:\n",
        "    predictions_sum+=model.predict(X_test)\n",
        "  predictions=np.argmax(predictions_sum/len(models),axis=1)\n",
        "  return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JlvMXC4Fmlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation_averaging(X_train_copy,y_train_copy,cv,epochs):\n",
        "  scores=[]\n",
        "  for i in range(cv):\n",
        "    X_train_cv,X_test_cv ,y_train_cv, y_test_cv=train_test_split(X_train_copy, y_train_copy,test_size=1./cv,random_state=i)\n",
        "    models=train_all_nets(X_train_cv,y_train_cv,10,epochs)\n",
        "    #models2=train_all_nets(X_train_cv,y_train_cv,10,epochs[1])\n",
        "    #models3=train_all_nets(X_train_cv,y_train_cv,10,epochs1[2])\n",
        "    predictions=predict(models,X_test_cv)\n",
        "\n",
        "    score=balanced_accuracy_score(y_test_cv,predictions)\n",
        "    print(score)\n",
        "    scores.append(score)\n",
        "  \n",
        "  print(\"cv average: \", np.mean(scores))\n",
        "  print(\"cv std: \", np.std(scores))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbh0wLhdOjC2",
        "colab_type": "code",
        "outputId": "c261df3c-fb86-4fd1-c737-d120008a4f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cross_validation_averaging(X_train_copy,y_train_copy,10,[10,20,30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "0.6987387387387388\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "0.7193777041332966\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "0.7010619924300885\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "0.7294895736072208\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "0.7037054010853397\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "0.6703379934719634\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw3VtESeOr6v",
        "colab_type": "code",
        "outputId": "b2634b6a-cb7a-41c3-904c-75e2cb34d470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"ok\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUZd-fzl0teT",
        "colab_type": "code",
        "outputId": "a15447a6-d56e-499e-b71c-4fd51640dfb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "models=train_all_nets(X_train_copy,y_train_copy,10,[10,20,30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n",
            "just trained 3 nets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AENjuiEau6u",
        "colab_type": "code",
        "outputId": "bbcb260e-1239-43c5-bec5-947723fe4f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.sequential.Sequential at 0x7fa3841222e8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa37be9a828>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa37b5d1f28>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa379c12b00>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa378b40dd8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa378209978>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa3768b4cc0>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa375f797b8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa3746269e8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa373cee5c0>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa372bb6128>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa371250f28>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa370914b00>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa36f7df668>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa36e6a2208>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa36cdb5c50>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa36c47a7b8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa36b33f358>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa369981f60>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa36889c828>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa367f603c8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa366e25f60>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa365cebac8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa364bb0668>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa363a74240>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa362178358>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa361833eb8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa35fe7aac8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa35f5435f8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa35e407198>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa35d2c7d68>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa35c18c898>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa35a889518>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa359f4a128>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa358e0ac18>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa357cd37b8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa356b9b390>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa355a5ceb8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa35491fa58>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa3537e56a0>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa351e71d68>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa351591908>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa3504564e0>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa34f31b048>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa34e1ddba8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa34c826780>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa34b6e92e8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa34adaee48>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa349c73a58>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa348b3b588>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa347a05128>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa346152c88>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa34580c7b8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa3446cc358>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa34358ff60>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa342455a58>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa340a9d5f8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa340162240>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa33f024cf8>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa33dee8898>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa33cdb3470>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa33bc75f98>,\n",
              " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fa33ab3cb38>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9bgd0sgZQGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions=predict(models,X_test_copy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyXTNr7xj3BA",
        "colab_type": "code",
        "outputId": "0f9da584-9cf1-45c7-d38e-8bad1ab75f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "submission(predictions,sample,\"64-nets-model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    id  y\n",
            "0  0.0  1\n",
            "1  1.0  0\n",
            "2  2.0  1\n",
            "3  3.0  2\n",
            "4  4.0  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPVyLXB4kJuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}